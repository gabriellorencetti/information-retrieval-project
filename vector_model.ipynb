{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    regex = re.compile('[^a-zA-Z0-9\\s]')\n",
    "    text_returned = re.sub(regex, '', text)\n",
    "    return text_returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_all_unique_words_and_freq(dict_global, words):\n",
    "    for word in set(words):\n",
    "        if word in dict_global.keys():\n",
    "            dict_global[word] += words.count(word)\n",
    "        else:\n",
    "            dict_global[word] = words.count(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered tokens: ['to', 'do', 'is', 'to', 'be', 'to', 'be', 'is', 'to', 'do']\n",
      "Filtered tokens: ['to', 'be', 'or', 'not', 'to', 'be', 'i', 'am', 'what', 'i', 'am']\n",
      "Filtered tokens: ['i', 'think', 'therefore', 'i', 'am', 'do', 'be', 'do', 'be', 'do']\n",
      "Filtered tokens: ['do', 'do', 'do', 'da', 'da', 'da', 'let', 'it', 'be', 'let', 'it', 'be']\n",
      "['am', 'be', 'da', 'do', 'i', 'is', 'it', 'let', 'not', 'or', 'therefore', 'think', 'to', 'what']\n"
     ]
    }
   ],
   "source": [
    "file_folder = 'data/*'\n",
    "dict_global = {}\n",
    "words_in_doc = {}\n",
    "docs_mapping = []\n",
    "N = 0\n",
    "for file in sorted(glob.glob(file_folder)):\n",
    "    filename = file\n",
    "    file = open(file, \"r\")\n",
    "    text = file.read()    \n",
    "    text = remove_special_characters(text)\n",
    "    words = word_tokenize(text)\n",
    "    words = [word.lower() for word in words]\n",
    "    finding_all_unique_words_and_freq(dict_global, words)\n",
    "    idx = os.path.basename(filename)    \n",
    "    docs_mapping.append(idx)\n",
    "    words_in_doc[N] = words\n",
    "    print('Filtered tokens:', words)\n",
    "    N += 1\n",
    "    file.close()\n",
    "\n",
    "unique_words_all = sorted(set(dict_global.keys()))\n",
    "print(unique_words_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, docId, freq):\n",
    "        self.freq = freq\n",
    "        self.doc = docId\n",
    "        self.next = None\n",
    "    \n",
    "    def __str__(self):        \n",
    "        return 'doc:' + str(self.doc) + ', freq:' + str(self.freq)\n",
    "\n",
    "class LinkedList:\n",
    "    def __init__(self):\n",
    "        self.head = None\n",
    "        self.tail = None\n",
    "        self.n_docs = 0\n",
    "    \n",
    "    def print_list(self):\n",
    "        aux = self.head\n",
    "        while aux:\n",
    "            print(aux)\n",
    "            aux = aux.next\n",
    "    \n",
    "    def get_doclist(self):\n",
    "        l = []\n",
    "        aux = self.head\n",
    "        while aux:\n",
    "            l.append([aux.doc, aux.freq])\n",
    "            aux = aux.next\n",
    "        return l\n",
    "    \n",
    "    def add_doc(self, doc, freq):\n",
    "        node = Node(doc, freq)        \n",
    "        if self.head == None:\n",
    "            self.head = node        \n",
    "        else:\n",
    "            self.tail.next = node\n",
    "        self.tail = node\n",
    "        self.n_docs += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc:0, freq:2\n",
      "doc:2, freq:3\n",
      "doc:3, freq:3\n"
     ]
    }
   ],
   "source": [
    "linked_list_data = {}\n",
    "for word in unique_words_all:\n",
    "    linked_list_data[word] = LinkedList()\n",
    "\n",
    "for docID in range(len(docs_mapping)):\n",
    "    words = words_in_doc[docID]\n",
    "    for word in set(words):\n",
    "        linked_list_data[word].add_doc(docID, words.count(word))        \n",
    "\n",
    "linked_list_data['do'].print_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['doc1.txt', 'doc2.txt', 'doc3.txt', 'doc4.txt']\n",
      "am [0. 2. 1. 0.]\n",
      "be [2. 2. 2. 2.]\n",
      "da [0. 0. 0. 3.]\n",
      "do [2. 0. 3. 3.]\n",
      "i [0. 2. 2. 0.]\n",
      "is [2. 0. 0. 0.]\n",
      "it [0. 0. 0. 2.]\n",
      "let [0. 0. 0. 2.]\n",
      "not [0. 1. 0. 0.]\n",
      "or [0. 1. 0. 0.]\n",
      "therefore [0. 0. 1. 0.]\n",
      "think [0. 0. 1. 0.]\n",
      "to [4. 2. 0. 0.]\n",
      "what [0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(docs_mapping)\n",
    "m = np.zeros((len(unique_words_all), len(docs_mapping)))\n",
    "for i in range(len(unique_words_all)):\n",
    "    word = unique_words_all[i]\n",
    "    postings = linked_list_data[word].get_doclist()\n",
    "    ni = linked_list_data[word].n_docs\n",
    "    for node in postings:\n",
    "        docID = node[0]\n",
    "        freq = node[1]\n",
    "        m[i, docID] = freq\n",
    "    print(word, m[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['doc1.txt', 'doc2.txt', 'doc3.txt', 'doc4.txt']\n",
      "am [0. 2. 1. 0.]\n",
      "be [0. 0. 0. 0.]\n",
      "da [0.       0.       0.       5.169925]\n",
      "do [0.830075   0.         1.07285637 1.07285637]\n",
      "i [0. 2. 2. 0.]\n",
      "is [4. 0. 0. 0.]\n",
      "it [0. 0. 0. 4.]\n",
      "let [0. 0. 0. 4.]\n",
      "not [0. 2. 0. 0.]\n",
      "or [0. 2. 0. 0.]\n",
      "therefore [0. 0. 2. 0.]\n",
      "think [0. 0. 2. 0.]\n",
      "to [3. 2. 0. 0.]\n",
      "what [0. 2. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(docs_mapping)\n",
    "m = np.zeros((len(unique_words_all), len(docs_mapping)))\n",
    "for i in range(len(unique_words_all)):\n",
    "    word = unique_words_all[i]\n",
    "    postings = linked_list_data[word].get_doclist()\n",
    "    ni = linked_list_data[word].n_docs\n",
    "    idf = math.log2(N/ni)\n",
    "    for node in postings:\n",
    "        docID = node[0]\n",
    "        freq = node[1]\n",
    "        m[i, docID] = (1 + math.log2(freq))*idf\n",
    "    print(word, m[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.068434127344514, 4.898979485566356, 3.761784256839167, 7.738161623767062]\n"
     ]
    }
   ],
   "source": [
    "norm = np.sum(m**2, axis=0)\n",
    "norm = [math.sqrt(norm[i]) for i in range(len(norm))]\n",
    "print(norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.        0.        0.        0.4150375 0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        1.        0.       ]\n"
     ]
    }
   ],
   "source": [
    "query = 'to do'\n",
    "query = remove_special_characters(query)\n",
    "query = word_tokenize(query)\n",
    "query = [q.lower() for q in query]\n",
    "q_vector = np.zeros(len(unique_words_all))\n",
    "for i in range(len(unique_words_all)):\n",
    "    word = unique_words_all[i]\n",
    "    if word in query:\n",
    "        ni = linked_list_data[word].n_docs\n",
    "        idf = math.log2(N/ni)\n",
    "        q_vector[i] = (1 + math.log2(query.count(word)))*idf\n",
    "print(q_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc1.txt 0.6598709123142042\n",
      "doc2.txt 0.4082482904638631\n",
      "doc3.txt 0.11836819852778786\n",
      "doc4.txt 0.05754281796914421\n"
     ]
    }
   ],
   "source": [
    "ranking = np.zeros(N)\n",
    "for j in range(N):\n",
    "    ranking[j] = np.dot(m[:,j], q_vector) / norm[j]\n",
    "    print(docs_mapping[j], ranking[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
