{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "Rec Info.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.10 64-bit"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "!pip install -r requirements.txt"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /home/henriq/.local/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (3.6.2)\n",
            "Requirement already satisfied: matplotlib in /home/henriq/.local/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (3.4.2)\n",
            "Requirement already satisfied: scipy in /home/henriq/.local/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (1.7.0)\n",
            "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk->-r requirements.txt (line 1)) (7.0)\n",
            "Requirement already satisfied: regex in /home/henriq/.local/lib/python3.8/site-packages (from nltk->-r requirements.txt (line 1)) (2021.7.6)\n",
            "Requirement already satisfied: joblib in /home/henriq/.local/lib/python3.8/site-packages (from nltk->-r requirements.txt (line 1)) (1.0.1)\n",
            "Requirement already satisfied: tqdm in /home/henriq/.local/lib/python3.8/site-packages (from nltk->-r requirements.txt (line 1)) (4.61.2)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/lib/python3/dist-packages (from matplotlib->-r requirements.txt (line 2)) (2.4.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/henriq/.local/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 2)) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /home/henriq/.local/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: numpy>=1.16 in /home/henriq/.local/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 2)) (1.21.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /home/henriq/.local/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /home/henriq/.local/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 2)) (8.2.0)\n",
            "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from cycler>=0.10->matplotlib->-r requirements.txt (line 2)) (1.14.0)\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "source": [
        "import re\n",
        "import os\n",
        "import gc\n",
        "import math\n",
        "import glob\n",
        "import nltk\n",
        "import shutil\n",
        "import numpy as np\n",
        "from os import listdir\n",
        "from zipfile import ZipFile\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from os.path import isfile, join\n",
        "from scipy.sparse import csc_matrix\n",
        "from collections import defaultdict\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords as stopWords\n",
        "from nltk.tokenize import word_tokenize as wordTokenize"
      ],
      "outputs": [],
      "metadata": {
        "id": "PsognIg80bkc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "source": [
        "np.seterr('raise')\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stopWordsSet = set(stopWords.words('english'))\n",
        "ps = PorterStemmer()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /home/henriq/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /home/henriq/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "metadata": {
        "id": "z-01mRUW0dQd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "778b6992-c5b0-4c59-ba56-15cf00639fd9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nh8b41d6AgwP",
        "outputId": "22cdd838-600c-45a3-903a-1bd689c7be39"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "source": [
        "class Node:\n",
        "    def __init__(self, docId, freq):\n",
        "        self.freq = freq\n",
        "        self.doc = docId\n",
        "        self.next = None\n",
        "    \n",
        "    def __str__(self):        \n",
        "        return 'doc:' + str(self.doc) + ', freq:' + str(self.freq)\n",
        "\n",
        "class LinkedList:\n",
        "    def __init__(self):\n",
        "        self.head = None\n",
        "        self.tail = None\n",
        "        self.n_docs = 0\n",
        "    \n",
        "    def print_list(self):\n",
        "        aux = self.head\n",
        "        while aux:\n",
        "            print(aux)\n",
        "            aux = aux.next\n",
        "    \n",
        "    def get_doclist(self):\n",
        "        l = []\n",
        "        aux = self.head\n",
        "        while aux:\n",
        "            l.append([aux.doc, aux.freq])\n",
        "            aux = aux.next\n",
        "        return l\n",
        "    \n",
        "    def add_doc(self, doc, freq):\n",
        "        node = Node(doc, freq)        \n",
        "        if self.head == None:\n",
        "            self.head = node        \n",
        "        else:\n",
        "            self.tail.next = node\n",
        "        self.tail = node\n",
        "        self.n_docs += 1"
      ],
      "outputs": [],
      "metadata": {
        "id": "hBKUKpRm0gGX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "source": [
        "def removeSpecialCharacters(text):\n",
        "    regex = re.compile('[^a-zA-Z0-9\\s]')\n",
        "    return re.sub(regex, '', text)\n",
        "\n",
        "def tokenizeAndRemoveSpecialCharacters(text):\n",
        "    return wordTokenize(removeSpecialCharacters(text))\n",
        "\n",
        "def listsIntersection(lst1, lst2):\n",
        "    return list(set(lst1) & set(lst2))\n",
        "\n",
        "def computeNorm(word, maxFreq, linkedListData, wordsInDocument, itf, docsIds):\n",
        "    sum = 0\n",
        "    docs = linkedListData[word].get_doclist()\n",
        "\n",
        "    for doc in wordsInDocument.keys():\n",
        "        if doc in docs:\n",
        "            freq = docs[1]\n",
        "        else:\n",
        "            freq = 0\n",
        "        a = pow((0.5 + 0.5 * (freq / maxFreq)), 2)\n",
        "        b = pow(itf[docsIds[doc]], 2)\n",
        "        sum = sum + a * b\n",
        "\n",
        "    return math.sqrt(sum)\n",
        "\n",
        "def getMaxFreq(word, linkedListData):\n",
        "    maxFreq = 0\n",
        "    docs = linkedListData[word].get_doclist()\n",
        "\n",
        "    for doc in docs:\n",
        "        freq = doc[1]\n",
        "        if freq > maxFreq:\n",
        "            maxFreq = freq\n",
        "    return maxFreq\n",
        "\n",
        "def getFileText(text):\n",
        "    try:\n",
        "        text = re.search('<TEXT>(.+?)</TEXT>', text, flags=re.DOTALL).group(1)\n",
        "    except AttributeError: # <TEXT></TEXT> not found in the original string\n",
        "        text = ''\n",
        "    return text\n",
        "\n",
        "def getFileName(text):\n",
        "    try:\n",
        "        name = re.search('<DOCNO>(.+?)</DOCNO>', text, flags=re.DOTALL).group(1)\n",
        "    except AttributeError: # <DOCNO></DOCNO> not found in the original string\n",
        "        name = ''\n",
        "    return name\n",
        "\n",
        "def filterText(removeStopWords, removeStemmingWords, value):\n",
        "    words = []\n",
        "    for w in value:\n",
        "        if removeStopWords == True and w not in stopWordsSet: # stop words\n",
        "            if removeStemmingWords == True:\n",
        "                w = ps.stem(w)  # stemming\n",
        "        elif removeStopWords == False:\n",
        "            if removeStemmingWords == True:\n",
        "                w = ps.stem(w)  # stemming\n",
        "        words.append(w)\n",
        "    return words\n",
        "\n",
        "def readDocs(path):\n",
        "    numDocs = 0\n",
        "    docsIds = {}\n",
        "    docsNames = []\n",
        "    wordsInDocument = {}\n",
        "\n",
        "    for folder in glob.glob(path):\n",
        "        for subfolder in glob.glob(folder + '/*'):\n",
        "            for directory in glob.glob(subfolder + '/*'):\n",
        "                for subdir in glob.glob(directory):\n",
        "                    with open(subdir, \"r\") as doc:\n",
        "                        text = doc.read()\n",
        "                        \n",
        "                        fileName = getFileName(text)\n",
        "                        \n",
        "                        wordsInDocument[fileName] = getFileText(text)\n",
        "                        \n",
        "                        wordsInDocument[fileName] = tokenizeAndRemoveSpecialCharacters(wordsInDocument[fileName].lower())\n",
        "                        \n",
        "                        # atribuindo um id pra cada documento\n",
        "                        docsIds[fileName] = numDocs\n",
        "\n",
        "                        # salvando nomes dos arquivos\n",
        "                        docsNames.append(fileName)\n",
        "                        \n",
        "                        numDocs = numDocs + 1\n",
        "                doc.close()\n",
        "\n",
        "    return numDocs, docsIds, docsNames, wordsInDocument\n",
        "\n",
        "def readQueriesDoc(path):\n",
        "    \"\"\"Lê o documento en.topics e armazena as queries, e seus respectivos ids, que serão usados\"\"\"\n",
        "\n",
        "    queries = defaultdict()\n",
        "    with open(path) as file:\n",
        "        text = file.read()\n",
        "        splitted = text.split(\"\\n\\n\")\n",
        "\n",
        "    for line in splitted:\n",
        "        if 'topics' in line: continue\n",
        "        id = re.search(r'<num>(.*?)</num>', line).group(1)\n",
        "        query = re.search(r'<title>(.*?)</title>.', line, flags=re.DOTALL).group(1)\n",
        "        query += re.search(r'<narr>(.*?)</narr>', line, flags=re.DOTALL).group(1)\n",
        "        query = removeSpecialCharacters(query)\n",
        "        queries[id] = wordTokenize(query)\n",
        "        queries[id] = filterText(True, True, queries[id])\n",
        "\n",
        "    return queries\n",
        "\n",
        "def prob_model(wordsInDocument, query, linkedListData, numDocs, uniqueWords):\n",
        "    \"\"\"\n",
        "    Modelo probabilistico\n",
        "    \"\"\"\n",
        "\n",
        "    # Seleção inicial\n",
        "    initial_sel_answer = {}\n",
        "    \n",
        "    for doc in wordsInDocument.keys():\n",
        "        words = wordsInDocument[doc]\n",
        "        commonWordsBetweenQueryAndDoc = listsIntersection(query, words)\n",
        "        relevance = 0\n",
        "        for ki in commonWordsBetweenQueryAndDoc:\n",
        "            numDocsWithKi = linkedListData[ki].n_docs\n",
        "            relevance += math.log10((numDocs + 0.5)/(numDocsWithKi + 0.5))\n",
        "            if relevance > 0:\n",
        "                initial_sel_answer[doc] = relevance\n",
        "    \n",
        "    # verifica qual a maior relevancia encontrada\n",
        "    # na seleção inicial dentre os documentos\n",
        "    max = 0\n",
        "    doc = ''\n",
        "    for item in initial_sel_answer.keys():\n",
        "        if initial_sel_answer[item] > max:\n",
        "            max = initial_sel_answer[item]\n",
        "            doc = item\n",
        "            \n",
        "    # Ranqueamento final\n",
        "\n",
        "    relevantDocs = []\n",
        "    numRelevantDocs = {}\n",
        "    \n",
        "    sorted_answer = sorted(initial_sel_answer.items(), key = lambda x: x[1], reverse = True)\n",
        "\n",
        "    relevantDocs.append(sorted_answer[0][0])\n",
        "    \n",
        "    final_rank_answer = {}\n",
        "    \n",
        "    for word in query:\n",
        "        if word not in uniqueWords:\n",
        "            continue\n",
        "        postings = linkedListData[word].head\n",
        "        numRelevantDocs[word] = 0\n",
        "        while postings:\n",
        "            if postings.doc in relevantDocs:\n",
        "                numRelevantDocs[word] += 1\n",
        "            postings = postings.next\n",
        "    \n",
        "    numRelevantDocsToQuery = len(relevantDocs)\n",
        "    \n",
        "    for doc in wordsInDocument.keys():\n",
        "        words = wordsInDocument[doc]\n",
        "        commonWordsBetweenQueryAndDoc = listsIntersection(query, words)\n",
        "        relevance = 0\n",
        "    \n",
        "        for ki in commonWordsBetweenQueryAndDoc:\n",
        "            numDocsWithKi = linkedListData[ki].n_docs\n",
        "            numRelevantDocsWithKi = numRelevantDocs[ki]\n",
        "            \n",
        "            numerator = ((numRelevantDocsWithKi + 0.5) * (numDocs - numDocsWithKi - numRelevantDocsToQuery + numRelevantDocsWithKi + 0.5))\n",
        "\n",
        "            denominator = ((numRelevantDocsToQuery - numRelevantDocsWithKi + 0.5) * (numDocsWithKi - numRelevantDocsWithKi + 0.5))\n",
        "\n",
        "            if numerator <= 0 or denominator <= 0: relevance += 0\n",
        "            else: relevance += math.log10(numerator / denominator)\n",
        "\n",
        "        final_rank_answer[doc] = relevance\n",
        "\n",
        "    # final_rank_answer\n",
        "\n",
        "    # Se os dois prints tiverem o mesmo valor, significa que\n",
        "    # o documento com mais relevância na seleção inicial\n",
        "    # é o mesmo na seleção final\n",
        "\n",
        "    # max = 0\n",
        "    # for item in final_rank_answer.keys():\n",
        "    #     if final_rank_answer[item] > max:\n",
        "    #         max = final_rank_answer[item]\n",
        "\n",
        "    # if final_rank_answer[relevantDocs[0]] == max:\n",
        "    #     print(\"\\nResultados iguais = \", final_rank_answer[relevantDocs[0]])\n",
        "\n",
        "    return initial_sel_answer, final_rank_answer\n",
        "\n",
        "def vect_model(wordsIds, uniqueWords, numDocs, linkedListData, query, docsNames, docsIds):\n",
        "    \"\"\"\n",
        "    Modelo Vetorial\n",
        "\n",
        "    Iremos utilizar a ponderação de termos TF-IDF, que considera a frequência de cada termo e a frequência inversa de documento.\n",
        "\n",
        "    Alguns termos possuem maior importância semântica para representar determinado elemento. Por exemplo, um termo que aparece em todos os documentos indexados não nos auxilia em nada no rankeamento dos mesmos, da mesma forma que um termo raro pode ser de extrema importância para identificar um determinado assunto.\n",
        "\n",
        "    \"\"\"\n",
        "    # Ponderação TF-IDF\n",
        "\n",
        "    m = np.zeros((len(uniqueWords), numDocs))\n",
        "    \n",
        "    for word in uniqueWords:\n",
        "        postings = linkedListData[word].get_doclist()\n",
        "        \n",
        "        idf = math.log2(numDocs/ linkedListData[word].n_docs)\n",
        "    \n",
        "        for node in postings:\n",
        "    \n",
        "            docName = node[0]\n",
        "            freq = node[1]\n",
        "    \n",
        "            # tf-idf = (1+log2(freq))*idf, se freq > 0\n",
        "            if freq > 0:\n",
        "              m[wordsIds[word], docsIds[docName]] = (1 + math.log2(freq)) * idf\n",
        "            # tf-idf = 0, caso contrario\n",
        "            #   m[wordsIds[word], docsIds[docName]] = 0\n",
        "\n",
        "    \"\"\"Normalização pelo tamanho dos documentos\"\"\"\n",
        "    m = m**2\n",
        "    norm = np.sum(m, axis=0)\n",
        "    norm = [math.sqrt(norm[i]) for i in range(len(norm))]\n",
        "\n",
        "    \"\"\"Realizando a ponderação TF-IDF na query\"\"\"\n",
        "\n",
        "    i = 0\n",
        "    queryVector = np.zeros(len(uniqueWords))\n",
        "    \n",
        "    for word in uniqueWords:\n",
        "        if word in query:\n",
        "            idf = math.log2(numDocs/ linkedListData[word].n_docs)\n",
        "            queryVector[i] = (1 + math.log2(query.count(word))) * idf\n",
        "            i += 1\n",
        "\n",
        "    \"\"\"Ranqueamento\"\"\"\n",
        "    \n",
        "    ranking = {}\n",
        "    \n",
        "    for docName in docsNames:\n",
        "        ranking[docName] = np.dot(m[:,docsIds[docName]], queryVector) / norm[docsIds[docName]]\n",
        "    \n",
        "    sorted_ranking = {k: v for k, v in sorted(ranking.items(), key = lambda item: item[1], reverse=True)}\n",
        "\n",
        "    del m\n",
        "    gc.collect()\n",
        "\n",
        "    return sorted_ranking\n",
        "\n",
        "def query_expansion(numDocs, wordsInDocument, docsIds, uniqueWords, linkedListData, query, wordsIds):\n",
        "    \"\"\"\n",
        "    Expansão de consultas\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    \"\"\"Frequência inversa de termo (itf)\"\"\"\n",
        "\n",
        "    # Frequencia inversa de termo (itf) pra cada documento j\n",
        "    # Calculo dele é dado por:\n",
        "    # numerador = nro de termos distintos em tds os docs\n",
        "    # denominador = nro de termos distintos no doc j\n",
        "    # itf(doc j) = numerador / denominador\n",
        "    itf = [0] * numDocs\n",
        "\n",
        "    for doc in wordsInDocument.keys():\n",
        "        uniqueWordsInDoc = set(wordsInDocument[doc])\n",
        "        if(len(uniqueWordsInDoc) == 0): \n",
        "            continue\n",
        "        docId = docsIds[doc]\n",
        "        itf[docId] = math.log2(len(uniqueWords) / len(uniqueWordsInDoc))\n",
        "\n",
        "    \"\"\"Pesagem dos documentos\"\"\"\n",
        "\n",
        "    w = np.zeros((len(uniqueWords), numDocs))\n",
        "    \n",
        "    for word in uniqueWords:\n",
        "        maxFreq = getMaxFreq(word, linkedListData)\n",
        "        norm = computeNorm(word, maxFreq, linkedListData, wordsInDocument, itf, docsIds)\n",
        "    \n",
        "        for docs in linkedListData[word].get_doclist():\n",
        "            fileName = docs[0]\n",
        "            freq = docs[1]\n",
        "            a = (0.5 + 0.5 * (freq / maxFreq))\n",
        "            b = itf[docsIds[fileName]]\n",
        "            w[wordsIds[word]][docsIds[fileName]] = (a * b) / norm\n",
        "\n",
        "    \"\"\"Similaridade\"\"\"\n",
        "\n",
        "    # Similaridade\n",
        "    c = np.dot(w, w.transpose())\n",
        "\n",
        "    del w\n",
        "    gc.collect()\n",
        "\n",
        "    \"\"\"Pesagem da consulta\"\"\"\n",
        "\n",
        "    # Pesagem da consulta\n",
        "    q = [0] * len(query)\n",
        "    \n",
        "    uniqueWordsInQuery = set(query)\n",
        "    \n",
        "    i = 0\n",
        "    wordsInQueryIds = {}\n",
        "    for word in query:\n",
        "        wordsInQueryIds[word] = i\n",
        "        i = i + 1\n",
        "    \n",
        "    sum = 0\n",
        "    for i in range(len(itf)):\n",
        "        sum = sum + math.pow(itf[i], 2)\n",
        "    norm = math.sqrt(sum)\n",
        "    \n",
        "    for word in query:\n",
        "        q[wordsInQueryIds[word]] = math.log2(len(uniqueWords)/ len(uniqueWordsInQuery)) / norm\n",
        "\n",
        "    \"\"\"Combinação da similaridade com pesagem da consulta\"\"\"\n",
        "\n",
        "    result = {}\n",
        "    \n",
        "    # Combinação da similaridade com a pesagem da consulta\n",
        "    for word in uniqueWords:\n",
        "        similaridade = 0\n",
        "        result[word] = similaridade\n",
        "        for item in query:\n",
        "            a = q[wordsInQueryIds[item]]\n",
        "            if item not in uniqueWords:\n",
        "                b = 0\n",
        "            else:\n",
        "                b = c[wordsIds[item]][wordsIds[word]]\n",
        "            similaridade = similaridade + a * b\n",
        "            result[word] = similaridade\n",
        "\n",
        "    \"\"\"Resultado da expansão\"\"\"\n",
        "\n",
        "    sorted_result = {k: v for k, v in sorted(result.items(), key = lambda item: item[1], reverse=True)}\n",
        "\n",
        "    keys = list(sorted_result.keys())\n",
        "\n",
        "    return sorted_result\n",
        "\n",
        "def getRelDocs(path):\n",
        "    rel_docs = defaultdict()\n",
        "\n",
        "    with open(path) as file:\n",
        "        for line in file:\n",
        "            token = wordTokenize(line)\n",
        "            if token[3] == '1':\n",
        "                if int(token[0]) in rel_docs.keys():\n",
        "                    rel_docs[int(token[0])].append(token[2])\n",
        "                else:\n",
        "                    rel_docs[int(token[0])] = [token[2]]\n",
        "\n",
        "    return rel_docs\n",
        "\n",
        "def evaluation(queryId, rel_docs, answer, recall_I):\n",
        "    \"\"\"\n",
        "    Avaliação\n",
        "    \n",
        "    A avaliação serve para julgar o quão bem o sistema atende à necessidade de \n",
        "    informação do usuário. Estamos considerando precisão e revocação. \n",
        "    \"\"\"\n",
        "\n",
        "    intersec = 0\n",
        "    acessedDocs = 0\n",
        "    precisaoList = []\n",
        "    revocacaoList = []\n",
        "    queryId = int(queryId)\n",
        "\n",
        "    for rel_doc in answer.keys():\n",
        "        acessedDocs += 1\n",
        "        if rel_doc in rel_docs:\n",
        "            intersec += 1\n",
        "            precisaoList.append(intersec/acessedDocs)\n",
        "            revocacaoList.append(intersec/len(rel_docs))\n",
        "\n",
        "    if len(revocacaoList) == 0 or len(precisaoList) == 0:\n",
        "        revocacaoList.append(0)\n",
        "        precisaoList.append(0)\n",
        "\n",
        "    precision_I = np.interp(recall_I, revocacaoList, precisaoList)\n",
        "\n",
        "    return precision_I\n",
        "\n",
        "def median(precision):\n",
        "    med = np.zeros(11)\n",
        "    tam = 0\n",
        "    for prec in precision.keys():        \n",
        "        med += precision[prec]\n",
        "        tam +=1\n",
        "    med = med/tam\n",
        "\n",
        "    return med"
      ],
      "outputs": [],
      "metadata": {
        "id": "4brYo6Ihz-lx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "source": [
        "%%time\n",
        "\n",
        "docsPath = 'a/*'\n",
        "topicsPath = 'en.topics.76-125.2010.txt'\n",
        "queriesPath = 'en.qrels.76-125.2010.txt'\n",
        "\n",
        "numDocs, docsIds, docsNames, wordsInDocument = readDocs(docsPath)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 34.2 ms, sys: 0 ns, total: 34.2 ms\n",
            "Wall time: 34.2 ms\n"
          ]
        }
      ],
      "metadata": {
        "id": "XO2csXGO0mbH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "source": [
        "%%time\n",
        "\n",
        "dict = defaultdict(int)\n",
        "\n",
        "# Remoção de stopwords, radicalização e indexação\n",
        "for key, value in wordsInDocument.items():\n",
        "  wordsInDocument[key] = filterText(True, True, value)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 87.6 ms, sys: 0 ns, total: 87.6 ms\n",
            "Wall time: 87.4 ms\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "source": [
        "%%time\n",
        "\n",
        "for key, value in wordsInDocument.items():\n",
        "  for w in set(value): \n",
        "    dict[w] += 1"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.64 ms, sys: 0 ns, total: 1.64 ms\n",
            "Wall time: 1.65 ms\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "source": [
        "%%time\n",
        "\n",
        "i = 0\n",
        "wordsIds = {}\n",
        "linkedListData = {}\n",
        "uniqueWords = sorted(set(dict.keys()))\n",
        "\n",
        "# atribuindo um id pra cada palavra\n",
        "for word in uniqueWords:\n",
        "    wordsIds[word] = i\n",
        "    linkedListData[word] = LinkedList()\n",
        "    i = i + 1\n",
        "\n",
        "print(\"\\nFound %d unique words\" % len(uniqueWords))\n",
        "\n",
        "for doc in wordsInDocument.keys():\n",
        "    words = wordsInDocument[doc]\n",
        "    for word in set(words):\n",
        "        linkedListData[word].add_doc(doc, words.count(word))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Found 2571 unique words\n",
            "CPU times: user 53 ms, sys: 119 µs, total: 53.2 ms\n",
            "Wall time: 51.7 ms\n"
          ]
        }
      ],
      "metadata": {
        "id": "jFVSc0qRC7Ca"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "source": [
        "%%time\n",
        "\n",
        "print(\"\\nFound %d unique words\" % len(uniqueWords))\n",
        "\n",
        "for doc in wordsInDocument.keys():\n",
        "  words = wordsInDocument[doc]\n",
        "  for word in set(words):\n",
        "    linkedListData[word].add_doc(doc, words.count(word))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Found 2571 unique words\n",
            "CPU times: user 50.5 ms, sys: 0 ns, total: 50.5 ms\n",
            "Wall time: 49.4 ms\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "source": [
        "%%time\n",
        "\n",
        "queries = readQueriesDoc(topicsPath)\n",
        "rel_docs = getRelDocs(queriesPath)\n",
        "\n",
        "initial_sel_answer = defaultdict()\n",
        "final_rank_answer = defaultdict()\n",
        "\n",
        "ranked_vect_answer = defaultdict()\n",
        "\n",
        "expansion_answer = defaultdict()\n",
        "precisionProb = defaultdict()\n",
        "precisionVet = defaultdict()\n",
        "\n",
        "recall = np.linspace(0, 1, num=11, endpoint=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.34 s, sys: 2.23 ms, total: 1.34 s\n",
            "Wall time: 1.34 s\n"
          ]
        }
      ],
      "metadata": {
        "id": "7lYjhGjq0rGk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "source": [
        "%%time\n",
        "\n",
        "# Define qual query iremos trabalhar em cima\n",
        "for queryId in queries.keys():\n",
        "    query = queries[queryId]\n",
        "\n",
        "    initial_sel_answer[queryId], final_rank_answer[queryId] = prob_model(wordsInDocument, query, linkedListData, numDocs, uniqueWords)\n",
        "\n",
        "    ranked_vect_answer[queryId] = vect_model(wordsIds, uniqueWords, numDocs, linkedListData, query, docsNames, docsIds)\n",
        "\n",
        "    # expansion_answer[queryId] = query_expansion(numDocs, wordsInDocument, docsIds, uniqueWords, linkedListData, query, wordsIds)\n",
        "\n",
        "    precisionProb[queryId] = evaluation(queryId, rel_docs[int(queryId)], initial_sel_answer[queryId], recall)\n",
        "\n",
        "    precisionVet[queryId] = evaluation(queryId, rel_docs[int(queryId)], ranked_vect_answer[queryId], recall)\n",
        "\n",
        "precisionProbMedian = median(precisionProb)\n",
        "precisionVetMedian = median(precisionVet)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3.12 s, sys: 0 ns, total: 3.12 s\n",
            "Wall time: 3.14 s\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "source": [
        "plt.plot(recall, precisionProbMedian, linestyle='-', marker='o', color='b')\n",
        "plt.plot(recall, precisionVetMedian, linestyle='-', marker='o', color='r')\n",
        "plt.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "metadata": {
        "id": "8hsAVZWD11D_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "f8a842c2-9524-4948-91c8-19a4473efbc8"
      }
    }
  ]
}